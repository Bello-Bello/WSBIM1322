<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 9 Dimensionality reduction | Bioinformatics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain." />


<meta name="author" content="Laurent Gatto" />

<meta name="date" content="2019-11-11" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain.">

<title>Chapter 9 Dimensionality reduction | Bioinformatics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Bioinformatics<p><p class="author">Laurent Gatto</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preamble</a>
<a href="sec-refresher.html"><span class="toc-section-number">1</span> R refresher</a>
<a href="sec-vis.html"><span class="toc-section-number">2</span> Data visualisation</a>
<a href="sec-obj.html"><span class="toc-section-number">3</span> High-level data structures</a>
<a href="sec-biostrings.html"><span class="toc-section-number">4</span> Manipulating sequences with Biostrings</a>
<a href="sec-norm.html"><span class="toc-section-number">5</span> Data normalisation: centring, scaling, quantile normalisation</a>
<a href="sec-mlintro.html"><span class="toc-section-number">6</span> Introduction to statistical machine learning</a>
<a href="sec-testing.html"><span class="toc-section-number">7</span> Hypothesis testing</a>
<a href="sec-ul.html"><span class="toc-section-number">8</span> Unsupervised learning</a>
<a id="active-page" href="sec-dimred.html"><span class="toc-section-number">9</span> Dimensionality reduction</a><ul class="toc-sections">
<li class="toc"><a href="#introduction-4"> Introduction</a></li>
<li class="toc"><a href="#lower-dimensional-projections"> Lower-dimensional projections</a></li>
<li class="toc"><a href="#the-new-linear-combinations"> The new linear combinations</a></li>
<li class="toc"><a href="#summary-and-application"> Summary and application</a></li>
<li class="toc"><a href="#visualisation"> Visualisation</a></li>
<li class="toc"><a href="#pre-processing-and-missing-values-with-pca"> Pre-processing and missing values with PCA</a></li>
<li class="toc"><a href="#the-full-pca-workflow"> The full PCA workflow</a></li>
</ul>
<a href="sec-sl.html"><span class="toc-section-number">10</span> Supervised learning</a>
<a href="sec-biovis.html"><span class="toc-section-number">11</span> Visualising biomolecular data</a>
<a href="sec-ccl.html"><span class="toc-section-number">12</span> Conclusions</a>
<a href="sec-si.html"><span class="toc-section-number">13</span> Session information</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="sec:dimred" class="section level1">
<h1>
<span class="header-section-number">Chapter 9</span> Dimensionality reduction</h1>
<div id="introduction-4" class="section level2">
<h2>
<span class="header-section-number">9.1</span> Introduction</h2>
<p>In this chapter, we are going to learn about dimensionality reduction, also called ordination. The goal of dimensionality reduction is to transform a high-dimensinal data into data of lesser dimensions while minimise the loss of information.</p>
<p>Dimensionality reduction is used as a data transformation technique for input to other machine learning methods such as classifications, or as a very efficient visualisation technique, which is the use case we will focus on here. We are going to focus on a widerly used method called Principal Component Analysis (PCA).</p>
<p>We are going to use the following dataset to illustrate some important concepts that are scale and central to PCA. The small dataset show below represents the measurement of genes <em>x</em> and <em>y</em> in 20 samples. We will be using the scaled and centred version of this data.</p>
<div class="figure">
<span id="fig:xyplot"></span>
<p class="caption marginnote shownote">
Figure 9.1: Raw (left) and scale/centred (right) expression data for genes <em>x</em> and <em>y</em> in 20 samples
</p>
<img src="WSBIM1322_files/figure-html/xyplot-1.png" alt="Raw (left) and scale/centred (right) expression data for genes *x* and *y* in 20 samples" width="1056">
</div>
</div>
<div id="lower-dimensional-projections" class="section level2">
<h2>
<span class="header-section-number">9.2</span> Lower-dimensional projections</h2>
<p>The goal of dimensionality reduction is to reduce the number of dimensions in a way that the new data remains useful. One way to reduce a 2-dimensional data is by projecting the data onto a lines. Below, we project our data on the x and y axes. These are called <strong>linear projections</strong>.</p>
<div class="figure">
<span id="fig:linproj"></span>
<p class="caption marginnote shownote">
Figure 9.2: Projection of the data on the x (left) and y (right) axes.
</p>
<img src="WSBIM1322_files/figure-html/linproj-1.png" alt="Projection of the data on the x (left) and y (right) axes." width="1056">
</div>
<p>In general, and in particular in the projections above, we loose information when reducing the number of dimensions (above, from 2 (plane) to 1 (line)). In the first example above (left), we loose all the measurements of gene <em>y</em>. In the second example (right), we loose all the measurements of gene <em>x</em>.</p>
<p>The goal of dimensionality reduction is to limit this loss.</p>
<p>We know already about <strong>linear regression</strong>. Below, we use the <code>lm</code> function to regress <em>y</em> onto <em>x</em> (left) and <em>x</em> onto <em>y</em> (right). These regression lines give us an approximate linear relationship between the expression of genes <em>x</em> and <em>y</em>. The relationship differs depending on which gene we choose to be the predictor adn the response.</p>
<div class="figure">
<span id="fig:linreg"></span>
<p class="caption marginnote shownote">
Figure 9.3: Regression of y onto x (left) minimisises the sums of squares of vertical residuals (red). Regression of x onto y (right) minimisises the sums of squares of horizontal residuals (orange).
</p>
<img src="WSBIM1322_files/figure-html/linreg-1.png" alt="Regression of y onto x (left) minimisises the sums of squares of vertical residuals (red). Regression of x onto y (right) minimisises the sums of squares of horizontal residuals (orange)." width="1056">
</div>
<p>We now want a line that minimises distances in both directions, as shown below. This line, called <strong>principal compoment</strong>, is also the ones that maximises the variance of the projections along itself.</p>
<div class="figure">
<span id="fig:pxaex"></span>
<p class="caption marginnote shownote">
Figure 9.4: The first prinicpal component minimises the sum of squares of the orthogonal projections.
</p>
<img src="WSBIM1322_files/figure-html/pxaex-1.png" alt="The first prinicpal component minimises the sum of squares of the orthogonal projections." width="1056">
</div>
<p>The second principal component is then chosen to be orthogonal to the first one. In our case above, there is only one possibility.</p>
<div class="figure">
<span id="fig:pxaex2"></span>
<p class="caption marginnote shownote">
Figure 9.5: The second prinicpal component is orthogonal to the second one.
</p>
<img src="WSBIM1322_files/figure-html/pxaex2-1.png" alt="The second prinicpal component is orthogonal to the second one." width="1056">
</div>
<p>In the example above the variance, the variance along the PCs are 1.77 and 0.23. The first one explains 88.6% or that variance, and the second one merely 11.4%. This is also reflected in the different scales along the x and y axis.</p>
<p>To account for these differences in variation along the different PCs, it is better to represent a PCA plot as a rectangle, using an aspect ratio that is illustrative of the respective variances.</p>
<div class="figure">
<span id="fig:pxaex3"></span>
<p class="caption marginnote shownote">
Figure 9.6: Final principal component analysis of the data.
</p>
<img src="WSBIM1322_files/figure-html/pxaex3-1.png" alt="Final principal component analysis of the data." width="960">
</div>
</div>
<div id="the-new-linear-combinations" class="section level2">
<h2>
<span class="header-section-number">9.3</span> The new linear combinations</h2>
<p>Principal components are linear combinations of the variables that were originally measured, they provide a new coordinate system. The PC in the previous example is a linear combination of <em>gene_x</em> and <em>gene_y</em>, more specifically</p>
<p><span class="math display">\[ PC = c_{1} ~ gene_{x} + c_{2} ~ gene_{y} \]</span></p>
<p>It has coefficients <span class="math inline">\((c_1, c_2)\)</span>, also called loading.</p>
<p>PCA in general will find linear combinations of the original variables. These new linear combinations will maximise the variance of the data.</p>
</div>
<div id="summary-and-application" class="section level2">
<h2>
<span class="header-section-number">9.4</span> Summary and application</h2>
<p>Principal Component Analysis (PCA) is a technique that transforms the original n-dimensional data into a new space.</p>
<ul>
<li><p>These new dimensions are linear combinations of the original data, i.e. they are composed of proportions of the original variables.</p></li>
<li><p>Along these new dimensions, called principal components, the data expresses most of its variability along the first PC, then second, …</p></li>
<li><p>Principal components are orthogonal to each other, i.e. non-correlated.</p></li>
</ul>
<div class="figure fullwidth">
<span id="fig:pcaex"></span>
<img src="WSBIM1322_files/figure-html/pcaex-1.png" alt="Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right)." width="1152"><p class="caption marginnote shownote">
Figure 9.7: Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right).
</p>
</div>
<p>In R, we can use the <code>prcomp</code> function. A summary of the <code>prcomp</code> output shows that along PC1 along, we are able to retain close to 92% of the total variability in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca_xy &lt;-<span class="st"> </span><span class="kw">prcomp</span>(xy)
<span class="kw">summary</span>(pca_xy)</code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2
## Standard deviation     1.3308 0.4784
## Proportion of Variance 0.8856 0.1144
## Cumulative Proportion  0.8856 1.0000</code></pre>
<p>This <code>pca_xy</code> variable is an object of class <code>prcomp</code>. To learn what it contains, we can look at its structure with <code>str</code> and read the <code>?prcomp</code> manual page.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(pca_xy)</code></pre></div>
<pre><code>## List of 5
##  $ sdev    : num [1:2] 1.331 0.478
##  $ rotation: num [1:2, 1:2] 0.707 0.707 -0.707 0.707
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:2] "gene_x" "gene_y"
##   .. ..$ : chr [1:2] "PC1" "PC2"
##  $ center  : Named num [1:2] 4.05e-16 1.53e-17
##   ..- attr(*, "names")= chr [1:2] "gene_x" "gene_y"
##  $ scale   : logi FALSE
##  $ x       : num [1:20, 1:2] -2.728 -2.4 -2.257 -0.945 -0.14 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : NULL
##   .. ..$ : chr [1:2] "PC1" "PC2"
##  - attr(*, "class")= chr "prcomp"</code></pre>
<p>We are going to focus on two elements:</p>
<ul>
<li>
<strong>sdev</strong> contains the standard deviations along the respective PCs (as also displayed in the summary). From these, we can compute the variances, the percentages of variance explained by the individual PCs, and the cumulative variances.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(var &lt;-<span class="st"> </span>pca_xy<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 1.7711407 0.2288593</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(pve &lt;-<span class="st"> </span>var<span class="op">/</span><span class="kw">sum</span>(var))</code></pre></div>
<pre><code>## [1] 0.8855704 0.1144296</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cumsum</span>(pve)</code></pre></div>
<pre><code>## [1] 0.8855704 1.0000000</code></pre>
<ul>
<li>
<strong>x</strong> contains the coordinates of the data along the PCs. These are the values we could use to produce the PCA plot as above by hand.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca_xy<span class="op">$</span>x</code></pre></div>
<pre><code>##               PC1         PC2
##  [1,] -2.72848772  0.17019003
##  [2,] -2.40018291  0.11371158
##  [3,] -2.25654153 -0.17987834
##  [4,] -0.94491834  0.14224604
##  [5,] -0.14044618  0.86760695
##  [6,] -0.85917747  0.06485259
##  [7,] -1.26714805 -0.38570171
##  [8,] -0.13426336  0.01962124
##  [9,]  0.14678704  0.14527416
## [10,]  1.08677349  0.67252884
## [11,]  0.93793985  0.43069778
## [12,]  0.62416628  0.08630825
## [13,]  0.05477564 -0.56143151
## [14,]  1.20838962  0.46257985
## [15,]  0.58230063 -0.44200381
## [16,]  1.48091889  0.42011071
## [17,]  1.13374563 -0.12764440
## [18,]  1.28218805 -0.32631519
## [19,]  1.46938699 -0.32768480
## [20,]  0.72379344 -1.24506826</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(pca_xy<span class="op">$</span>x)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaplot-1.png" width="672"></p>
</div>
<div id="visualisation" class="section level2">
<h2>
<span class="header-section-number">9.5</span> Visualisation</h2>
<p>A <strong>biplot</strong> features all original points re-mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs. Feature vectors that are in the same direction in PC space are also correlated in the original data space.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(pca_xy)</code></pre></div>
<div class="figure">
<span id="fig:biplot"></span>
<p class="caption marginnote shownote">
Figure 9.8: A biplot shows both the variables (arrows) and observations of the PCA analysis.
</p>
<img src="WSBIM1322_files/figure-html/biplot-1.png" alt="A biplot shows both the variables (arrows) and observations of the PCA analysis." width="672">
</div>
<p>One important piece of information when using PCA is the proportion of variance explained along the PCs (see above), in particular when dealing with high dimensional data, as PC1 and PC2 (that are generally used for visualisation), might only account for an insufficient proportion of variance to be relevant on their own. This can be visualised on a <strong>screeplot</strong>, that can be produced with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(pca_xy)</code></pre></div>
<div class="figure">
<span id="fig:screeplot"></span>
<p class="caption marginnote shownote">
Figure 9.9: Screeplot showing the variances for the PCs.
</p>
<img src="WSBIM1322_files/figure-html/screeplot-1.png" alt="Screeplot showing the variances for the PCs." width="672">
</div>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<ul>
<li><p>Repeat the analysis above with the <code>xy0</code> data, that you can load from the <code>rWSBIM1207</code> data. First scale it, then repeat the PCA analysis as shown above.</p></li>
<li><p>Load the <code>cptac</code> data, perform the PCA analysis and interpret it. Also produce a PCA for PCs 2 and 3.</p></li>
<li><p>In the exercise above, the PCA was performed on the features (proteins). Transpose the data and produce a PCA of the samples.</p></li>
</ul>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<div class="solution">
<p class="solution-begin">
► Solution<span id="sol-start-41" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-41', 'sol-start-41')"></span>
</p>
<div id="sol-body-41" class="solution-body" style="display: none;">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xy &lt;-<span class="st"> </span><span class="kw">scale</span>(xy0)
pca1 &lt;-<span class="st"> </span><span class="kw">prcomp</span>(xy)
<span class="kw">summary</span>(pca1)</code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2
## Standard deviation     1.3308 0.4784
## Proportion of Variance 0.8856 0.1144
## Cumulative Proportion  0.8856 1.0000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">screeplot</span>(pca1)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex1-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(pca1)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex1-2.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"MSnbase"</span>)
<span class="kw">data</span>(cptac)
x &lt;-<span class="st"> </span><span class="kw">exprs</span>(cptac)
pca2 &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x)
<span class="kw">summary</span>(pca2)</code></pre></div>
<pre><code>## Importance of components:
##                              PC1       PC2       PC3       PC4       PC5
## Standard deviation     1.211e+07 2.208e+06 1.419e+06 1.249e+06 1.106e+06
## Proportion of Variance 9.347e-01 3.110e-02 1.283e-02 9.950e-03 7.810e-03
## Cumulative Proportion  9.347e-01 9.658e-01 9.787e-01 9.886e-01 9.964e-01
##                              PC6
## Standard deviation     7.484e+05
## Proportion of Variance 3.570e-03
## Cumulative Proportion  1.000e+00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">screeplot</span>(pca2)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex2-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(pca2)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex2-2.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(pca2<span class="op">$</span>x[, <span class="dv">2</span><span class="op">:</span><span class="dv">3</span>])</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex2-3.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(pca2, <span class="dt">choices =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">3</span>)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/pcaex2-4.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_t &lt;-<span class="st"> </span><span class="kw">t</span>(x)
pca3 &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x_t)
<span class="kw">summary</span>(pca3)</code></pre></div>
<pre><code>## Importance of components:
##                              PC1       PC2       PC3       PC4       PC5
## Standard deviation     6.322e+07 4.081e+07 4.015e+07 3.441e+07 2.157e+07
## Proportion of Variance 4.479e-01 1.867e-01 1.807e-01 1.327e-01 5.215e-02
## Cumulative Proportion  4.479e-01 6.345e-01 8.152e-01 9.478e-01 1.000e+00
##                             PC6
## Standard deviation     4.76e-08
## Proportion of Variance 0.00e+00
## Cumulative Proportion  1.00e+00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">screeplot</span>(pca3)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/unnamed-chunk-50-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(pca3)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/unnamed-chunk-50-2.png" width="672"></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="pre-processing-and-missing-values-with-pca" class="section level2">
<h2>
<span class="header-section-number">9.6</span> Pre-processing and missing values with PCA</h2>
<p>We haven’t looked at other <code>prcomp</code> parameters, other that the first one, <code>x</code>. There are two other ones that are or importance, in particular in the light of the section on pre-processing above, which are <code>center</code> and <code>scale.</code>. The former is set to <code>TRUE</code> by default, while the second one is set the <code>FALSE</code>.</p>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<p>Perform a PCA analysis on the <code>mtcars</code> dataset with and without scaling. Compare and interpret the results.</p>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<div class="solution">
<p class="solution-begin">
► Solution<span id="sol-start-42" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-42', 'sol-start-42')"></span>
</p>
<div id="sol-body-42" class="solution-body" style="display: none;">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">biplot</span>(<span class="kw">prcomp</span>(mtcars, <span class="dt">scale =</span> <span class="ot">FALSE</span>), <span class="dt">main =</span> <span class="st">"No scaling"</span>)  ## 1
<span class="kw">biplot</span>(<span class="kw">prcomp</span>(mtcars, <span class="dt">scale =</span> <span class="ot">TRUE</span>), <span class="dt">main =</span> <span class="st">"With scaling"</span>) ## 2</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/scalepcasol-1.png" width="672"></p>
<p>Without scaling, <code>disp</code> and <code>hp</code> are the features with the highest loadings along PC1 and 2 (all others are negligible), which are also those with the highest units of measurement. Scaling removes this effect.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Real datasets often come with <em>missing values</em>. In R, these should be encoded using <code>NA</code>. Unfortunately, PCA cannot deal with missing values, and observations containing <code>NA</code> values will be dropped automatically. This is a viable solution only when the proportion of missing values is low. Otherwise, it is possible to impute missing values (which often requires great care) or use an implementation of PCA such as <em>non-linear iterative partial least squares</em> (NIPALS), that support missing values.</p>
<p>Finally, we should be careful when using categorical data in any of the unsupervised methods. Categories are generally represented as factors, which are encoded as integer levels, and might give the impression that a distance between levels is a relevant measure (which it is not, unless the factors are ordered). In such situations, categorical data can be dropped, or it is possible to encode categories as binary <em>dummy variables</em>. For example, if we have 3 categories, say <code>A</code>, <code>B</code> and <code>C</code>, we would create two dummy variables to encode the categories as:</p>
<pre><code>##   x y
## A 1 0
## B 0 1
## C 0 0</code></pre>
<p>so that the distance between each category are approximately equal to 1.</p>
</div>
<div id="the-full-pca-workflow" class="section level2">
<h2>
<span class="header-section-number">9.7</span> The full PCA workflow</h2>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="sec-ul.html"><button class="btn btn-default">Previous</button></a>
<a href="sec-sl.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2019-11-11
</p>
</div>
</div>



</body>
</html>
