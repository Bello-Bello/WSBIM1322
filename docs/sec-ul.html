<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 8 Unsupervised learning | Bioinformatics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain." />


<meta name="author" content="Laurent Gatto" />

<meta name="date" content="2019-11-09" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain.">

<title>Chapter 8 Unsupervised learning | Bioinformatics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Bioinformatics<p><p class="author">Laurent Gatto</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preamble</a>
<a href="sec-refresher.html"><span class="toc-section-number">1</span> R refresher</a>
<a href="sec-vis.html"><span class="toc-section-number">2</span> Data visualisation</a>
<a href="sec-obj.html"><span class="toc-section-number">3</span> High-level data structures</a>
<a href="sec-biostrings.html"><span class="toc-section-number">4</span> Manipulating sequences with Biostrings</a>
<a href="sec-norm.html"><span class="toc-section-number">5</span> Data normalisation: centring, scaling, quantile normalisation</a>
<a href="sec-mlintro.html"><span class="toc-section-number">6</span> Introduction to statistical machine learning</a>
<a href="sec-testing.html"><span class="toc-section-number">7</span> Hypothesis testing</a>
<a id="active-page" href="sec-ul.html"><span class="toc-section-number">8</span> Unsupervised learning</a><ul class="toc-sections">
<li class="toc"><a href="#introduction-3"> Introduction</a></li>
<li class="toc"><a href="#k-means-clustering"> k-means clustering</a></li>
<li class="toc"><a href="#hierarchical-clustering"> Hierarchical clustering</a></li>
<li class="toc"><a href="#pre-processing"> Pre-processing</a></li>
<li class="toc"><a href="#additional-exercises-6"> Additional exercises</a></li>
</ul>
<a href="sec-dimred.html"><span class="toc-section-number">9</span> Dimensionality reduction</a>
<a href="sec-sl.html"><span class="toc-section-number">10</span> Supervised learning</a>
<a href="sec-biovis.html"><span class="toc-section-number">11</span> Visualising biomolecular data</a>
<a href="sec-ccl.html"><span class="toc-section-number">12</span> Conclusions</a>
<a href="sec-si.html"><span class="toc-section-number">13</span> Session information</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="sec:ul" class="section level1">
<h1>
<span class="header-section-number">Chapter 8</span> Unsupervised learning</h1>
<div id="introduction-3" class="section level2">
<h2>
<span class="header-section-number">8.1</span> Introduction</h2>
<p>In <strong>unsupervised learning</strong> (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between</p>
<ul>
<li>
<strong>Clustering</strong>, where the goal is to find homogeneous subgroups within the data; the grouping is based on similiarities (or distance) between observations. The result of a clustering algorithm is to group the observations (features) into distinct (generally non-overlapping) groups. These groups are, even of imperfect (i.e. ignoring intermediate states) are often very useful in interpretation and assessment of the data.</li>
</ul>
<p>There is however one important caveat with clustering: clustering algorithms will always find clusters, even when none exists<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> See the additional exercise for an illustration of this risk.</span>. It is thus essential to critically assess and validate (for example using existing domain knowledge) the results of any clustering algorithm.</p>
<ul>
<li>
<strong>Dimensionality reduction</strong> (see chapter <a href="sec-dimred.html#sec:dimred">9</a>), where the goal is to summarise the data in a reduced number of dimensations, i.e. using a reduced number of (newly computer) variables. A typical example in omics is to summarise (and visualise) the relation between samples along two dimensions (which can easily be plotted) even if the data consists of tens of thousands of dimensions ( genes). Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning.</li>
</ul>
<p>UML presents specific challenges and benefits:</p>
<ul>
<li>Challenge: there is no single goal in UML</li>
<li>Benefit: there is generally much more unlabelled data available than labelled data.</li>
</ul>
<p>In this chapter we will:</p>
<ul>
<li>Learn how to use a widely used non-parametric clustering algorithms <strong>k-means</strong>.</li>
<li>Learn how to use reucrsive clustering approaches known as <strong>hierarchical clustering</strong>.</li>
<li>
<li>Observe the clustering parameters and distance metrics influence of outputs of these algorithms.</li>
</ul>
</div>
<div id="k-means-clustering" class="section level2">
<h2>
<span class="header-section-number">8.2</span> k-means clustering</h2>
<p>The k-means clustering algorithms aims at partitioning <em>n</em> observations into a fixed number of <em>k</em> clusters. The algorithm will find homogeneous clusters.</p>
<p>In R, we use</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stats<span class="op">::</span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</code></pre></div>
<p>where</p>
<ul>
<li>
<code>x</code> is a numeric data matrix</li>
<li>
<code>centers</code> is the pre-defined number of clusters</li>
<li>the k-means algorithm has a random component and can be repeated <code>nstart</code> times to improve the returned model</li>
</ul>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<blockquote>
<p>Challenge:</p>
<ul>
<li>To learn about k-means, let’s use the <code>iris</code> dataset with the sepal and petal length variables only (to facilitate visualisation). Create such a data matrix and name it <code>x</code>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Run the k-means algorithm on the newly generated data <code>x</code>, save the results in a new variable <code>cl</code>, and explore its output when printed.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>The actual results of the algorithms, i.e. the cluster membership can be accessed in the <code>clusters</code> element of the clustering result output. Use it to colour the inferred clusters to generate a figure like that shown below.</li>
</ul>
</blockquote>
<div class="figure">
<span id="fig:solkmplot"></span>
<p class="caption marginnote shownote">
Figure 8.1: k-means algorithm on sepal and petal lengths
</p>
<img src="WSBIM1322_files/figure-html/solkmplot-1.png" alt="k-means algorithm on sepal and petal lengths" width="672">
</div>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">i &lt;-<span class="st"> </span><span class="kw">grep</span>(<span class="st">"Length"</span>, <span class="kw">names</span>(iris))
x &lt;-<span class="st"> </span>iris[, i]
cl &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)
<span class="kw">plot</span>(x, <span class="dt">col =</span> cl<span class="op">$</span>cluster)</code></pre></div>
<p></p>
<div id="how-does-k-means-work" class="section level3">
<h3>
<span class="header-section-number">8.2.1</span> How does k-means work</h3>
<p><strong>Initialisation</strong>: randomly assign class membership</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12</span>)
init &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">3</span>, <span class="kw">nrow</span>(x), <span class="dt">replace =</span> <span class="ot">TRUE</span>)
<span class="kw">plot</span>(x, <span class="dt">col =</span> init)</code></pre></div>
<div class="figure">
<span id="fig:kmworksinit"></span>
<p class="caption marginnote shownote">
Figure 8.2: k-means random intialisation
</p>
<img src="WSBIM1322_files/figure-html/kmworksinit-1.png" alt="k-means random intialisation" width="672">
</div>
<p><strong>Iteration</strong>:</p>
<ol style="list-style-type: decimal">
<li>Calculate the centre of each subgroup as the average position of all observations is that subgroup.</li>
<li>Each observation is then assigned to the group of its nearest centre.</li>
</ol>
<p>It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(x, <span class="dt">col =</span> init)
centres &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) <span class="kw">colMeans</span>(x[init <span class="op">==</span><span class="st"> </span>i, ], ))
centres &lt;-<span class="st"> </span><span class="kw">t</span>(centres)
<span class="kw">points</span>(centres[, <span class="dv">1</span>], centres[, <span class="dv">2</span>], <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)

tmp &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">rbind</span>(centres, x))
tmp &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tmp)[, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]

ki &lt;-<span class="st"> </span><span class="kw">apply</span>(tmp, <span class="dv">1</span>, which.min)
ki &lt;-<span class="st"> </span>ki[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)]

<span class="kw">plot</span>(x, <span class="dt">col =</span> ki)
<span class="kw">points</span>(centres[, <span class="dv">1</span>], centres[, <span class="dv">2</span>], <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</code></pre></div>
<div class="figure">
<span id="fig:kmworksiter"></span>
<p class="caption marginnote shownote">
Figure 8.3: k-means iteration: calculate centers (left) and assign new cluster membership (right)
</p>
<img src="WSBIM1322_files/figure-html/kmworksiter-1.png" alt="k-means iteration: calculate centers (left) and assign new cluster membership (right)" width="1152">
</div>
<p><strong>Termination</strong>: Repeat iteration until no point changes its cluster membership.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif" alt="k-means convergence (credit Wikipedia)"><p class="caption">k-means convergence (credit Wikipedia)</p>
</div>
</div>
<div id="model-selection" class="section level3">
<h3>
<span class="header-section-number">8.2.2</span> Model selection</h3>
<p>Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest <em>total within cluster sum of squares (SS)</em>, is selected. The total within SS is calculated as:</p>
<p>For each cluster results:</p>
<ul>
<li>for each observation, determine the squared euclidean distance from observation to centre of cluster</li>
<li>sum all distances</li>
</ul>
<p>Note that this is a <strong>local minimum</strong>; there is no guarantee to obtain a global minimum.</p>
<blockquote>
<p>Challenge:</p>
<p>Repeat k-means on our <code>x</code> data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. Try the same with random data of identical dimensions.</p>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)
cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)
<span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 41  0
##   2 51  0  0
##   3  0  0 58</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)
cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)
<span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##    
##      1  2  3
##   1 41  0  0
##   2  0  0 58
##   3  0 51  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
xr &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="kw">prod</span>(<span class="kw">dim</span>(x))), <span class="dt">ncol =</span> <span class="kw">ncol</span>(x))
cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(xr, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)
cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(xr, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)
<span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 52  0
##   2  0  0 46
##   3 52  0  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">diffres &lt;-<span class="st"> </span>cl1<span class="op">$</span>cluster <span class="op">!=</span><span class="st"> </span>cl2<span class="op">$</span>cluster
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(xr, <span class="dt">col =</span> cl1<span class="op">$</span>cluster, <span class="dt">pch =</span> <span class="kw">ifelse</span>(diffres, <span class="dv">19</span>, <span class="dv">1</span>))
<span class="kw">plot</span>(xr, <span class="dt">col =</span> cl2<span class="op">$</span>cluster, <span class="dt">pch =</span> <span class="kw">ifelse</span>(diffres, <span class="dv">19</span>, <span class="dv">1</span>))</code></pre></div>
<div class="figure">
<span id="fig:selrep"></span>
<p class="caption marginnote shownote">
Figure 8.4: Different k-means results on the same (random) data
</p>
<img src="WSBIM1322_files/figure-html/selrep-1.png" alt="Different k-means results on the same (random) data" width="1152">
</div>
<p></p>
</div>
<div id="how-to-determine-the-number-of-clusters" class="section level3">
<h3>
<span class="header-section-number">8.2.3</span> How to determine the number of clusters</h3>
<ol style="list-style-type: decimal">
<li>Run k-means with <code>k=1</code>, <code>k=2</code>, …, <code>k=n</code>
</li>
<li>Record total within SS for each value of k.</li>
<li>Choose k at the <em>elbow</em> position, as illustrated below.</li>
</ol>
<p><img src="WSBIM1322_files/figure-html/kmelbow-1.png" width="672"></p>
<blockquote>
<p>Challenge</p>
<p>Calculate the total within sum of squares for k from 1 to 5 for our <code>x</code> test data, and reproduce the figure above.</p>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ks &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>
tot_within_ss &lt;-<span class="st"> </span><span class="kw">sapply</span>(ks, <span class="cf">function</span>(k) {
    cl &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, k, <span class="dt">nstart =</span> <span class="dv">10</span>)
    cl<span class="op">$</span>tot.withinss
})
<span class="kw">plot</span>(ks, tot_within_ss, <span class="dt">type =</span> <span class="st">"b"</span>)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/solkmelbow-1.png" width="672"></p>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>
<span class="header-section-number">8.3</span> Hierarchical clustering</h2>
<div id="how-does-hierarchical-clustering-work" class="section level3">
<h3>
<span class="header-section-number">8.3.1</span> How does hierarchical clustering work</h3>
<p><strong>Initialisation</strong>: Starts by assigning each of the n points its own cluster</p>
<p><strong>Iteration</strong></p>
<ol style="list-style-type: decimal">
<li>Find the two nearest clusters, and join them together, leading to n-1 clusters</li>
<li>Continue the cluster merging process until all are grouped into a single cluster</li>
</ol>
<p><strong>Termination:</strong> All observations are grouped within a single cluster.</p>
<div class="figure">
<span id="fig:hcldata"></span>
<p class="caption marginnote shownote">
Figure 8.5: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right).
</p>
<img src="WSBIM1322_files/figure-html/hcldata-1.png" alt="Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right)." width="1152">
</div>
<p>The results of hierarchical clustering are typically visualised along a <strong>dendrogram</strong>, where the distance between the clusters is proportional to the branch lengths.</p>
<div class="figure">
<span id="fig:hcldendro"></span>
<p class="caption marginnote shownote">
Figure 8.6: Visualisation of the hierarchical clustering results on a dendrogram
</p>
<img src="WSBIM1322_files/figure-html/hcldendro-1.png" alt="Visualisation of the hierarchical clustering results on a dendrogram" width="672">
</div>
<p>In R:</p>
<ul>
<li>Calculate the distance using <code>dist</code>, typically the Euclidean distance.</li>
<li>Hierarchical clustering on this distance matrix using <code>hclust</code>
</li>
</ul>
<blockquote>
<p>Challenge</p>
<p>Apply hierarchical clustering on the <code>iris</code> data and generate a dendrogram using the dedicated <code>plot</code> method.</p>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])
hcl &lt;-<span class="st"> </span><span class="kw">hclust</span>(d)
hcl</code></pre></div>
<pre><code>## 
## Call:
## hclust(d = d)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 150</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(hcl)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/hclsol-1.png" width="672"></p>
</div>
<div id="defining-clusters" class="section level3">
<h3>
<span class="header-section-number">8.3.2</span> Defining clusters</h3>
<p>After producing the hierarchical clustering result, we need to <em>cut the tree (dendrogram)</em> at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 1.5, with would produce 2 clusters.</p>
<div class="figure">
<span id="fig:cuthcl"></span>
<p class="caption marginnote shownote">
Figure 8.7: Cutting the dendrogram at height 1.5.
</p>
<img src="WSBIM1322_files/figure-html/cuthcl-1.png" alt="Cutting the dendrogram at height 1.5." width="672">
</div>
<p>In R we can us the <code>cutree</code> function to</p>
<ul>
<li>cut the tree at a specific height: <code>cutree(hcl, h = 1.5)</code>
</li>
<li>cut the tree to get a certain number of clusters: <code>cutree(hcl, k = 2)</code>
</li>
</ul>
<blockquote>
<p>Challenge</p>
<ul>
<li>Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting <code>h</code>.</li>
<li>Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting directly <code>k</code>, and verify that both provide the same results.</li>
</ul>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(hcl)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="fl">3.9</span>, <span class="dt">col =</span> <span class="st">"red"</span>)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/cuthclsol-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3
##  [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 2 2 2 2 2
## [106] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [141] 2 2 2 2 2 2 2 2 2 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cutree</span>(hcl, <span class="dt">h =</span> <span class="fl">3.9</span>)</code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3
##  [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 2 2 2 2 2
## [106] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [141] 2 2 2 2 2 2 2 2 2 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">identical</span>(<span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>), <span class="kw">cutree</span>(hcl, <span class="dt">h =</span> <span class="fl">3.9</span>))</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p></p>
<blockquote>
<p>Challenge</p>
<p>Using the same value <code>k = 3</code>, verify if k-means and hierarchical clustering produce the same results on the <code>iris</code> data.</p>
<p>Which one, if any, is correct?</p>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)
hcl &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]))
<span class="kw">table</span>(km<span class="op">$</span>cluster, <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>))</code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 34 28
##   2  0 38  0
##   3 50  0  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(iris<span class="op">$</span>Petal.Length, iris<span class="op">$</span>Sepal.Length, <span class="dt">col =</span> km<span class="op">$</span>cluster, <span class="dt">main =</span> <span class="st">"k-means"</span>)
<span class="kw">plot</span>(iris<span class="op">$</span>Petal.Length, iris<span class="op">$</span>Sepal.Length, <span class="dt">col =</span> <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>), <span class="dt">main =</span> <span class="st">"Hierarchical clustering"</span>)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/iris2algs-1.png" width="1152"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Checking with the labels provided with the iris data
<span class="kw">table</span>(iris<span class="op">$</span>Species, km<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa      0  0 50
##   versicolor 48  2  0
##   virginica  14 36  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(iris<span class="op">$</span>Species, <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>))</code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 23 27
##   virginica   0 49  1</code></pre>
<p></p>
</div>
</div>
<div id="pre-processing" class="section level2">
<h2>
<span class="header-section-number">8.4</span> Pre-processing</h2>
<p>Many of the machine learning methods that are regularly used are sensitive to difference scales. This applies to unsupervised methods as well as supervised methods, as we will see in the next chapter.</p>
<p>A typical way to pre-process the data prior to learning is to scale the data, or apply principal component analysis (next section). Scaling assures that all data columns have a mean of 0 and standard deviation of 1.</p>
<p>In R, scaling is done with the <code>scale</code> function.</p>
<blockquote>
<p>Challenge</p>
<p>Using the <code>mtcars</code> data as an example, verify that the variables are of different scales, then scale the data. To observe the effect different scales, compare the hierarchical clusters obtained on the original and scaled data.</p>
</blockquote>
<p><details></details></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(mtcars)</code></pre></div>
<pre><code>##        mpg        cyl       disp         hp       drat         wt 
##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250 
##       qsec         vs         am       gear       carb 
##  17.848750   0.437500   0.406250   3.687500   2.812500</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hcl1 &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(mtcars))
hcl2 &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(<span class="kw">scale</span>(mtcars)))
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(hcl1, <span class="dt">main =</span> <span class="st">"original data"</span>)
<span class="kw">plot</span>(hcl2, <span class="dt">main =</span> <span class="st">"scaled data"</span>)</code></pre></div>
<p><img src="WSBIM1322_files/figure-html/scalesol-1.png" width="1152"></p>
</div>
<div id="additional-exercises-6" class="section level2">
<h2>
<span class="header-section-number">8.5</span> Additional exercises</h2>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<p>An important caveat of clustering is that a clustering algorithm such as <em>k-means</em> will always find clusters, even when there is none. Illustrate this by creating a 2-dimensional random data by generation a matrix of 100 rows and 2 columns from <em>N(0, 1)</em> and searching for 2, 3, 4, 5 and 6 cluster. Visualise these results and assess whether they look convincing or not.</p>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="sec-testing.html"><button class="btn btn-default">Previous</button></a>
<a href="sec-dimred.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2019-11-09
</p>
</div>
</div>



</body>
</html>
