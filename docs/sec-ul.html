<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 9 Unsupervised learning: clustering | Bioinformatics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain." />


<meta name="author" content="Laurent Gatto" />

<meta name="date" content="2019-11-14" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Course material for the Bbioinformatics (WSBIM1322) course at UCLouvain.">

<title>Chapter 9 Unsupervised learning: clustering | Bioinformatics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Bioinformatics<p><p class="author">Laurent Gatto</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preamble</a>
<a href="sec-refresher.html"><span class="toc-section-number">1</span> R refresher</a>
<a href="sec-vis.html"><span class="toc-section-number">2</span> Data visualisation</a>
<a href="sec-obj.html"><span class="toc-section-number">3</span> High-level data structures</a>
<a href="sec-biostrings.html"><span class="toc-section-number">4</span> Manipulating sequences with Biostrings</a>
<a href="sec-norm.html"><span class="toc-section-number">5</span> Data normalisation: centring, scaling, quantile normalisation</a>
<a href="sec-mlintro.html"><span class="toc-section-number">6</span> Introduction to statistical machine learning</a>
<a href="sec-testing.html"><span class="toc-section-number">7</span> Hypothesis testing</a>
<a href="sec-dimred.html"><span class="toc-section-number">8</span> Unsupervised learning: dimensionality reduction</a>
<a id="active-page" href="sec-ul.html"><span class="toc-section-number">9</span> Unsupervised learning: clustering</a><ul class="toc-sections">
<li class="toc"><a href="#introduction-4"> Introduction</a></li>
<li class="toc"><a href="#k-means-clustering"> k-means clustering</a></li>
<li class="toc"><a href="#hierarchical-clustering"> Hierarchical clustering</a></li>
<li class="toc"><a href="#pre-processing"> Pre-processing</a></li>
<li class="toc"><a href="#additional-exercises-7"> Additional exercises</a></li>
</ul>
<a href="sec-sl.html"><span class="toc-section-number">10</span> Supervised learning</a>
<a href="sec-biovis.html"><span class="toc-section-number">11</span> Visualising biomolecular data</a>
<a href="sec-ccl.html"><span class="toc-section-number">12</span> Conclusions</a>
<a href="sec-si.html"><span class="toc-section-number">13</span> Session information</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="sec:ul" class="section level1">
<h1>
<span class="header-section-number">Chapter 9</span> Unsupervised learning: clustering</h1>
<p>This chapter has as objectives to</p>
<ul>
<li>introduce clustering methods;</li>
<li>provide the students with an understanding of kmeans and
hierarchical clustering;</li>
<li>show how to perform clustering;</li>
<li>provide real-life example of how to apply clustering on omics data.</li>
</ul>
<div id="introduction-4" class="section level2">
<h2>
<span class="header-section-number">9.1</span> Introduction</h2>
<p>After learing about dimensionality reduction and PCA, in this chapter
we will focus on <strong>clustering</strong>. The goal of clustering algorithms is
to find homogeneous subgroups within the data; the grouping is based
on similiarities (or distance) between observations. The result of a
clustering algorithm is to group the observations (features) into
distinct (generally non-overlapping) groups. These groups are, even of
imperfect (i.e. ignoring intermediate states) are often very useful in
interpretation and assessment of the data.</p>
<p>In this chapter we will:</p>
<ul>
<li>Learn how to use a widely used non-parametric clustering algorithms
<strong>k-means</strong>.</li>
<li>Learn how to use reucrsive clustering approaches known as
<strong>hierarchical clustering</strong>.</li>
<li>
<li>Observe the clustering parameters and distance metrics influence of
outputs of these algorithms.</li>
</ul>
</div>
<div id="k-means-clustering" class="section level2">
<h2>
<span class="header-section-number">9.2</span> k-means clustering</h2>
<p>The k-means clustering algorithms aims at partitioning <em>n</em>
observations into a fixed number of <em>k</em> clusters. The algorithm will
find homogeneous clusters.</p>
<p>In R, we use</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="sec-ul.html#cb343-1"></a>stats<span class="op">::</span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>where</p>
<ul>
<li>
<code>x</code> is a numeric data matrix</li>
<li>
<code>centers</code> is the pre-defined number of clusters</li>
<li>the k-means algorithm has a random component and can be repeated
<code>nstart</code> times to improve the returned model</li>
</ul>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<blockquote>
<p>Challenge:</p>
<ul>
<li>To learn about k-means, let’s use the <code>iris</code> dataset with the sepal and
petal length variables only (to facilitate visualisation). Create
such a data matrix and name it <code>x</code>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Run the k-means algorithm on the newly generated data <code>x</code>, save
the results in a new variable <code>cl</code>, and explore its output when
printed.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>The actual results of the algorithms, i.e. the cluster membership
can be accessed in the <code>clusters</code> element of the clustering result
output. Use it to colour the inferred clusters to generate a figure
like that shown below.</li>
</ul>
</blockquote>
<div class="figure">
<span id="fig:solkmplot"></span>
<p class="caption marginnote shownote">
Figure 9.1: k-means algorithm on sepal and petal lengths
</p>
<img src="WSBIM1322_files/figure-html/solkmplot-1.png" alt="k-means algorithm on sepal and petal lengths" width="672">
</div>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<details><div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="sec-ul.html#cb344-1"></a>i &lt;-<span class="st"> </span><span class="kw">grep</span>(<span class="st">"Length"</span>, <span class="kw">names</span>(iris))</span>
<span id="cb344-2"><a href="sec-ul.html#cb344-2"></a>x &lt;-<span class="st"> </span>iris[, i]</span>
<span id="cb344-3"><a href="sec-ul.html#cb344-3"></a>cl &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb344-4"><a href="sec-ul.html#cb344-4"></a><span class="kw">plot</span>(x, <span class="dt">col =</span> cl<span class="op">$</span>cluster)</span></code></pre></div>
</details><div id="how-does-k-means-work" class="section level3">
<h3>
<span class="header-section-number">9.2.1</span> How does k-means work</h3>
<p><strong>Initialisation</strong>: randomly assign class membership</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="sec-ul.html#cb345-1"></a><span class="kw">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb345-2"><a href="sec-ul.html#cb345-2"></a>init &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">3</span>, <span class="kw">nrow</span>(x), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb345-3"><a href="sec-ul.html#cb345-3"></a><span class="kw">plot</span>(x, <span class="dt">col =</span> init)</span></code></pre></div>
<div class="figure">
<span id="fig:kmworksinit"></span>
<p class="caption marginnote shownote">
Figure 9.2: k-means random intialisation
</p>
<img src="WSBIM1322_files/figure-html/kmworksinit-1.png" alt="k-means random intialisation" width="672">
</div>
<p><strong>Iteration</strong>:</p>
<ol style="list-style-type: decimal">
<li>Calculate the centre of each subgroup as the average position of
all observations is that subgroup.</li>
<li>Each observation is then assigned to the group of its nearest
centre.</li>
</ol>
<p>It’s also possible to stop the algorithm after a certain number of
iterations, or once the centres move less than a certain distance.</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="sec-ul.html#cb346-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb346-2"><a href="sec-ul.html#cb346-2"></a><span class="kw">plot</span>(x, <span class="dt">col =</span> init)</span>
<span id="cb346-3"><a href="sec-ul.html#cb346-3"></a>centres &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) <span class="kw">colMeans</span>(x[init <span class="op">==</span><span class="st"> </span>i, ], ))</span>
<span id="cb346-4"><a href="sec-ul.html#cb346-4"></a>centres &lt;-<span class="st"> </span><span class="kw">t</span>(centres)</span>
<span id="cb346-5"><a href="sec-ul.html#cb346-5"></a><span class="kw">points</span>(centres[, <span class="dv">1</span>], centres[, <span class="dv">2</span>], <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</span>
<span id="cb346-6"><a href="sec-ul.html#cb346-6"></a></span>
<span id="cb346-7"><a href="sec-ul.html#cb346-7"></a>tmp &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">rbind</span>(centres, x))</span>
<span id="cb346-8"><a href="sec-ul.html#cb346-8"></a>tmp &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tmp)[, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span>
<span id="cb346-9"><a href="sec-ul.html#cb346-9"></a></span>
<span id="cb346-10"><a href="sec-ul.html#cb346-10"></a>ki &lt;-<span class="st"> </span><span class="kw">apply</span>(tmp, <span class="dv">1</span>, which.min)</span>
<span id="cb346-11"><a href="sec-ul.html#cb346-11"></a>ki &lt;-<span class="st"> </span>ki[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)]</span>
<span id="cb346-12"><a href="sec-ul.html#cb346-12"></a></span>
<span id="cb346-13"><a href="sec-ul.html#cb346-13"></a><span class="kw">plot</span>(x, <span class="dt">col =</span> ki)</span>
<span id="cb346-14"><a href="sec-ul.html#cb346-14"></a><span class="kw">points</span>(centres[, <span class="dv">1</span>], centres[, <span class="dv">2</span>], <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure">
<span id="fig:kmworksiter"></span>
<p class="caption marginnote shownote">
Figure 9.3: k-means iteration: calculate centers (left) and assign new cluster membership (right)
</p>
<img src="WSBIM1322_files/figure-html/kmworksiter-1.png" alt="k-means iteration: calculate centers (left) and assign new cluster membership (right)" width="1152">
</div>
<p><strong>Termination</strong>: Repeat iteration until no point changes its cluster
membership.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif" alt=""><p class="caption">k-means convergence (credit Wikipedia)</p>
</div>
</div>
<div id="model-selection" class="section level3">
<h3>
<span class="header-section-number">9.2.2</span> Model selection</h3>
<p>Due to the random initialisation, one can obtain different clustering
results. When k-means is run multiple times, the best outcome,
i.e. the one that generates the smallest <em>total within cluster sum of
squares (SS)</em>, is selected. The total within SS is calculated as:</p>
<p>For each cluster results:</p>
<ul>
<li>for each observation, determine the squared euclidean distance from
observation to centre of cluster</li>
<li>sum all distances</li>
</ul>
<p>Note that this is a <strong>local minimum</strong>; there is no guarantee to obtain
a global minimum.</p>
<blockquote>
<p>Challenge:</p>
<p>Repeat k-means on our <code>x</code> data multiple times, setting the number of
iterations to 1 or greater and check whether you repeatedly obtain
the same results. Try the same with random data of identical
dimensions.</p>
</blockquote>
<details><div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="sec-ul.html#cb347-1"></a>cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb347-2"><a href="sec-ul.html#cb347-2"></a>cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb347-3"><a href="sec-ul.html#cb347-3"></a><span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</span></code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 41  0
##   2 51  0  0
##   3  0  0 58</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="sec-ul.html#cb349-1"></a>cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)</span>
<span id="cb349-2"><a href="sec-ul.html#cb349-2"></a>cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)</span>
<span id="cb349-3"><a href="sec-ul.html#cb349-3"></a><span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</span></code></pre></div>
<pre><code>##    
##      1  2  3
##   1 41  0  0
##   2  0  0 58
##   3  0 51  0</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="sec-ul.html#cb351-1"></a><span class="kw">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb351-2"><a href="sec-ul.html#cb351-2"></a>xr &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="kw">prod</span>(<span class="kw">dim</span>(x))), <span class="dt">ncol =</span> <span class="kw">ncol</span>(x))</span>
<span id="cb351-3"><a href="sec-ul.html#cb351-3"></a>cl1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(xr, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)</span>
<span id="cb351-4"><a href="sec-ul.html#cb351-4"></a>cl2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(xr, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)</span>
<span id="cb351-5"><a href="sec-ul.html#cb351-5"></a><span class="kw">table</span>(cl1<span class="op">$</span>cluster, cl2<span class="op">$</span>cluster)</span></code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 52  0
##   2  0  0 46
##   3 52  0  0</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="sec-ul.html#cb353-1"></a>diffres &lt;-<span class="st"> </span>cl1<span class="op">$</span>cluster <span class="op">!=</span><span class="st"> </span>cl2<span class="op">$</span>cluster</span>
<span id="cb353-2"><a href="sec-ul.html#cb353-2"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb353-3"><a href="sec-ul.html#cb353-3"></a><span class="kw">plot</span>(xr, <span class="dt">col =</span> cl1<span class="op">$</span>cluster, <span class="dt">pch =</span> <span class="kw">ifelse</span>(diffres, <span class="dv">19</span>, <span class="dv">1</span>))</span>
<span id="cb353-4"><a href="sec-ul.html#cb353-4"></a><span class="kw">plot</span>(xr, <span class="dt">col =</span> cl2<span class="op">$</span>cluster, <span class="dt">pch =</span> <span class="kw">ifelse</span>(diffres, <span class="dv">19</span>, <span class="dv">1</span>))</span></code></pre></div>
<div class="figure">
<span id="fig:selrep"></span>
<p class="caption marginnote shownote">
Figure 9.4: Different k-means results on the same (random) data
</p>
<img src="WSBIM1322_files/figure-html/selrep-1.png" alt="Different k-means results on the same (random) data" width="1152">
</div>
</details>
</div>
<div id="how-to-determine-the-number-of-clusters" class="section level3">
<h3>
<span class="header-section-number">9.2.3</span> How to determine the number of clusters</h3>
<ol style="list-style-type: decimal">
<li>Run k-means with <code>k=1</code>, <code>k=2</code>, …, <code>k=n</code>
</li>
<li>Record total within SS for each value of k.</li>
<li>Choose k at the <em>elbow</em> position, as illustrated below.</li>
</ol>
<p><img src="WSBIM1322_files/figure-html/kmelbow-1.png" width="672"></p>
<blockquote>
<p>Challenge</p>
<p>Calculate the total within sum of squares for k from 1 to 5 for our
<code>x</code> test data, and reproduce the figure above.</p>
</blockquote>
<details><div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="sec-ul.html#cb354-1"></a>ks &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span></span>
<span id="cb354-2"><a href="sec-ul.html#cb354-2"></a>tot_within_ss &lt;-<span class="st"> </span><span class="kw">sapply</span>(ks, <span class="cf">function</span>(k) {</span>
<span id="cb354-3"><a href="sec-ul.html#cb354-3"></a>    cl &lt;-<span class="st"> </span><span class="kw">kmeans</span>(x, k, <span class="dt">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb354-4"><a href="sec-ul.html#cb354-4"></a>    cl<span class="op">$</span>tot.withinss</span>
<span id="cb354-5"><a href="sec-ul.html#cb354-5"></a>})</span>
<span id="cb354-6"><a href="sec-ul.html#cb354-6"></a><span class="kw">plot</span>(ks, tot_within_ss, <span class="dt">type =</span> <span class="st">"b"</span>)</span></code></pre></div>
<img src="WSBIM1322_files/figure-html/solkmelbow-1.png" width="672"></details>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>
<span class="header-section-number">9.3</span> Hierarchical clustering</h2>
<div id="how-does-hierarchical-clustering-work" class="section level3">
<h3>
<span class="header-section-number">9.3.1</span> How does hierarchical clustering work</h3>
<p><strong>Initialisation</strong>: Starts by assigning each of the n points its own cluster</p>
<p><strong>Iteration</strong></p>
<ol style="list-style-type: decimal">
<li>Find the two nearest clusters, and join them together, leading to
n-1 clusters</li>
<li>Continue the cluster merging process until all are grouped into a
single cluster</li>
</ol>
<p><strong>Termination:</strong> All observations are grouped within a single cluster.</p>
<div class="figure">
<span id="fig:hcldata"></span>
<p class="caption marginnote shownote">
Figure 9.5: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right).
</p>
<img src="WSBIM1322_files/figure-html/hcldata-1.png" alt="Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right)." width="1152">
</div>
<p>The results of hierarchical clustering are typically visualised along
a <strong>dendrogram</strong>, where the distance between the clusters is
proportional to the branch lengths.</p>
<div class="figure">
<span id="fig:hcldendro"></span>
<p class="caption marginnote shownote">
Figure 9.6: Visualisation of the hierarchical clustering results on a dendrogram
</p>
<img src="WSBIM1322_files/figure-html/hcldendro-1.png" alt="Visualisation of the hierarchical clustering results on a dendrogram" width="672">
</div>
<p>In R:</p>
<ul>
<li>Calculate the distance using <code>dist</code>, typically the Euclidean
distance.</li>
<li>Hierarchical clustering on this distance matrix using <code>hclust</code>
</li>
</ul>
<blockquote>
<p>Challenge</p>
<p>Apply hierarchical clustering on the <code>iris</code> data and generate a
dendrogram using the dedicated <code>plot</code> method.</p>
</blockquote>
<details><div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="sec-ul.html#cb355-1"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb355-2"><a href="sec-ul.html#cb355-2"></a>hcl &lt;-<span class="st"> </span><span class="kw">hclust</span>(d)</span>
<span id="cb355-3"><a href="sec-ul.html#cb355-3"></a>hcl</span></code></pre></div>
<pre><code>## 
## Call:
## hclust(d = d)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 150</code></pre>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="sec-ul.html#cb357-1"></a><span class="kw">plot</span>(hcl)</span></code></pre></div>
<img src="WSBIM1322_files/figure-html/hclsol-1.png" width="672"></details>
</div>
<div id="defining-clusters" class="section level3">
<h3>
<span class="header-section-number">9.3.2</span> Defining clusters</h3>
<p>After producing the hierarchical clustering result, we need to <em>cut
the tree (dendrogram)</em> at a specific height to defined the
clusters. For example, on our test dataset above, we could decide to
cut it at a distance around 1.5, with would produce 2 clusters.</p>
<div class="figure">
<span id="fig:cuthcl"></span>
<p class="caption marginnote shownote">
Figure 9.7: Cutting the dendrogram at height 1.5.
</p>
<img src="WSBIM1322_files/figure-html/cuthcl-1.png" alt="Cutting the dendrogram at height 1.5." width="672">
</div>
<p>In R we can us the <code>cutree</code> function to</p>
<ul>
<li>cut the tree at a specific height: <code>cutree(hcl, h = 1.5)</code>
</li>
<li>cut the tree to get a certain number of clusters: <code>cutree(hcl, k = 2)</code>
</li>
</ul>
<blockquote>
<p>Challenge</p>
<ul>
<li>Cut the iris hierarchical clustering result at a height to obtain
3 clusters by setting <code>h</code>.</li>
<li>Cut the iris hierarchical clustering result at a height to obtain
3 clusters by setting directly <code>k</code>, and verify that both provide
the same results.</li>
</ul>
</blockquote>
<details><div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="sec-ul.html#cb358-1"></a><span class="kw">plot</span>(hcl)</span>
<span id="cb358-2"><a href="sec-ul.html#cb358-2"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="fl">3.9</span>, <span class="dt">col =</span> <span class="st">"red"</span>)</span></code></pre></div>
<p><img src="WSBIM1322_files/figure-html/cuthclsol-1.png" width="672"></p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="sec-ul.html#cb359-1"></a><span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3
##  [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3
##  [ reached getOption("max.print") -- omitted 50 entries ]</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="sec-ul.html#cb361-1"></a><span class="kw">cutree</span>(hcl, <span class="dt">h =</span> <span class="fl">3.9</span>)</span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3
##  [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3
##  [ reached getOption("max.print") -- omitted 50 entries ]</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="sec-ul.html#cb363-1"></a><span class="kw">identical</span>(<span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>), <span class="kw">cutree</span>(hcl, <span class="dt">h =</span> <span class="fl">3.9</span>))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</details><blockquote>
<p>Challenge</p>
<p>Using the same value <code>k = 3</code>, verify if k-means and hierarchical
clustering produce the same results on the <code>iris</code> data.</p>
<p>Which one, if any, is correct?</p>
</blockquote>
<details><div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="sec-ul.html#cb365-1"></a>km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb365-2"><a href="sec-ul.html#cb365-2"></a>hcl &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]))</span>
<span id="cb365-3"><a href="sec-ul.html#cb365-3"></a><span class="kw">table</span>(km<span class="op">$</span>cluster, <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 34 28
##   2  0 38  0
##   3 50  0  0</code></pre>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="sec-ul.html#cb367-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb367-2"><a href="sec-ul.html#cb367-2"></a><span class="kw">plot</span>(iris<span class="op">$</span>Petal.Length, iris<span class="op">$</span>Sepal.Length, <span class="dt">col =</span> km<span class="op">$</span>cluster, <span class="dt">main =</span> <span class="st">"k-means"</span>)</span>
<span id="cb367-3"><a href="sec-ul.html#cb367-3"></a><span class="kw">plot</span>(iris<span class="op">$</span>Petal.Length, iris<span class="op">$</span>Sepal.Length, <span class="dt">col =</span> <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>), <span class="dt">main =</span> <span class="st">"Hierarchical clustering"</span>)</span></code></pre></div>
<p><img src="WSBIM1322_files/figure-html/iris2algs-1.png" width="1152"></p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="sec-ul.html#cb368-1"></a><span class="co">## Checking with the labels provided with the iris data</span></span>
<span id="cb368-2"><a href="sec-ul.html#cb368-2"></a><span class="kw">table</span>(iris<span class="op">$</span>Species, km<span class="op">$</span>cluster)</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa      0  0 50
##   versicolor 48  2  0
##   virginica  14 36  0</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="sec-ul.html#cb370-1"></a><span class="kw">table</span>(iris<span class="op">$</span>Species, <span class="kw">cutree</span>(hcl, <span class="dt">k =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 23 27
##   virginica   0 49  1</code></pre>
</details>
</div>
</div>
<div id="pre-processing" class="section level2">
<h2>
<span class="header-section-number">9.4</span> Pre-processing</h2>
<p>Many of the machine learning methods that are regularly used are
sensitive to difference scales. This applies to unsupervised methods
as well as supervised methods, as we will see in the next chapter.</p>
<p>A typical way to pre-process the data prior to learning is to scale
the data, or apply principal component analysis (next section). Scaling
assures that all data columns have a mean of 0 and standard deviation of 1.</p>
<p>In R, scaling is done with the <code>scale</code> function.</p>
<blockquote>
<p>Challenge</p>
<p>Using the <code>mtcars</code> data as an example, verify that the variables are
of different scales, then scale the data. To observe the effect
different scales, compare the hierarchical clusters obtained on the
original and scaled data.</p>
</blockquote>
<details><div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="sec-ul.html#cb372-1"></a><span class="kw">colMeans</span>(mtcars)</span></code></pre></div>
<pre><code>##        mpg        cyl       disp         hp       drat         wt 
##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250 
##       qsec         vs         am       gear       carb 
##  17.848750   0.437500   0.406250   3.687500   2.812500</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="sec-ul.html#cb374-1"></a>hcl1 &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(mtcars))</span>
<span id="cb374-2"><a href="sec-ul.html#cb374-2"></a>hcl2 &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(<span class="kw">scale</span>(mtcars)))</span>
<span id="cb374-3"><a href="sec-ul.html#cb374-3"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb374-4"><a href="sec-ul.html#cb374-4"></a><span class="kw">plot</span>(hcl1, <span class="dt">main =</span> <span class="st">"original data"</span>)</span>
<span id="cb374-5"><a href="sec-ul.html#cb374-5"></a><span class="kw">plot</span>(hcl2, <span class="dt">main =</span> <span class="st">"scaled data"</span>)</span></code></pre></div>
<img src="WSBIM1322_files/figure-html/scalesol-1.png" width="1152"></details>
</div>
<div id="additional-exercises-7" class="section level2">
<h2>
<span class="header-section-number">9.5</span> Additional exercises</h2>
<div class="question">
<p class="question-begin">
► Question
</p>
<div class="question-body">
<p>An important caveat of clustering is that a clustering algorithm such
as <em>k-means</em> will always find clusters, even when there is
none. Illustrate this by creating a 2-dimensional random data by
generation a matrix of 100 rows and 2 columns from <em>N(0, 1)</em> and
searching for 2, 3, 4, 5 and 6 cluster. Visualise these results and assess
whether they look convincing or not.</p>
<p class="question-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="sec-dimred.html"><button class="btn btn-default">Previous</button></a>
<a href="sec-sl.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2019-11-14
</p>
</div>
</div>



</body>
</html>
