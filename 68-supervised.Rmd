# Supervised learning {#sec:sl}

The goal of this last chapter about machine learning will be
supervised learning, and in particular **classification**. In this
chapter, you will learn

- learn what classification is;
- learn about labelled, unlabelled, training and testing data;
- learn about and apply k-nearest neighbour, a simple non-parametric
  classifier;
- see why and how to use cross-validation;
- and learn about model complexity and generalisation.

## Introduction

Often, we faced with omics data, we have annotations for some of the
samples (or features) we have acquired:

- Among the 100s of patient tumours that were assayed using RNA
  sequencing of microarrays, we know the grade of the tumour for about
  half. We want to predict the grade of the other half using the gene
  expression profiles.
  
- We have performed a spatial proteomics experiment such as in
  Christoforou *et al.* [@Christoforou:2016] (see section
  \@ref(sec:dimred02)) and know the sub-cellular localisation of some
  proteins. We want to predict the sub-cellular localisation of the
  other proteins using the protein profiles.
  
  
In both of these examples, the quantitative data are the data that we
want to use to predict properties about samples or features; these are
called the **predictors**. The grade of the samples in the first
example and the protein sub-cellular localisation in the second one
are the **labels** that we have in some cases, and want to predcit
otherwise. We can thus split our data in two parts, depending whether
we have labels, or whether we want to predict them. The former are
called **labelled**, and the latter **unlabelled**.


```{r, echo = FALSE, fig.cap = "In supervised learning, the data are split in labelled or unlabelled data. The same applies when some of the columns are labelled.", fig.width = 12, fig.height = 5}
par(mfrow = c(1, 2), mar = c(0, 1, 2, 0), oma = c(0, 0, 0, 0))
rWSBIM1322:::sl_input_rows(main = "All data")
text(rep(0.8, 8), (1:8) + 0.5, 1:8)

rWSBIM1322:::blank_plot()
title(main = "Labelled and unlabelled data")
rWSBIM1322:::quant_data()
rect(9.2, 8, 10.2, 9, col = "steelblue")
rect(9.2, 7, 10.2, 8, col = "steelblue")
rect(9.2, 6, 10.2, 7, col = "red")
rect(9.2, 5, 10.2, 6, col = "red")
rect(9.2, 4, 10.2, 5, col = "green")
text(rep(0.8, 8), (1:8) + 0.5,
     c(2, 4, 7, 5, 1, 6, 3, 8))
```

In the figure above, the labels represent categories that need to be
inferred from the predictors. This class of supervised learning is
called **classification**. Classification are also split into **binary
classification** when there are only two classes, or **multi-label**
problem when, like above, there are more than two. The latter is a
generalisation of the binary task. When the annotations are continuous
values, the situation is referred to as a **regression** problem.

`r msmbstyle::question_begin()`

Load the `giris2` data from the `rWSBIM1322` package (requires version
>= 0.1.13). Identify the labelled and unlabelled data; how many are
there respectively.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r}
library("rWSBIM1322")
data(giris2)
summary(giris2)
table(is.na(giris2$GRADE))
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Visualise the `giris2` data on a PCA plot, highlighting the labels (or
absence thereof). Using the visualisation, will the classifying of the
unlabelled data will be difficult or easy? Motivate your aswer.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
```{r}
pca2 <- prcomp(giris2[, -5], scale = TRUE)
factoextra::fviz_pca_ind(pca2, col.ind = giris2$GRADE)
```
`r msmbstyle::solution_end()`

## K-nearest neighbours classifier

In this chapter, we'll use a simple, but useful classification
algorithm, k-nearest neighbours (kNN) to classify the *giris*
patients. We will use the `knn` function from the 
`r CRANpkg("class")` package.

K-nearest neighbours works by directly measuring the (Euclidean)
distance between observations and inferring the class of unlabelled data
from the class of its nearest neighbours. In the figure below, the
unlabelled instances *1* and *2* will be assigned classes *A* (blue)
and *B* (red) as their closest neighbours are red and blue,
respectively.

```{r knnex, echo=FALSE, fig.cap="Schematic illustrating the k nearest neighbors algorithm."}
p1 <- c(0, 0)
p2 <- c(0.7, 0.5)
x1 <- rbind(c(0.2, 0.2),
            c(-0.3, -0.8),
            c(-0.2, 1.3))
x2 <- rbind(c(1, 1),
            c(0.5, 0.7))
x3 <- c(1.5, -.9)
x <- rbind(p1, p2, x1, x2, x3)
col <- c("black", "black",
         rep("steelblue", 3),
         rep("red", 2),
         "darkgreen")

plot(x, pch = 19, col = col,
     cex = 5, xlab = "", ylab = "",
     xaxt = "n", yaxt = "n")
grid()
text(p1[1], p1[2], "1", col = "white", cex = 2)
text(p2[1], p2[2], "2", col = "white", cex = 2)
for (i in 1:3)
    segments(p1[1], p1[2],
             x1[i, 1], x1[i, 2],
             lty = "dotted",
             col = "steelblue")
segments(p2[1], p2[2],
         x1[1, 1], x1[1, 2],
         lty = "dotted",
         col = "steelblue")
for (i in 1:2)
    segments(p2[1], p2[2],
             x2[i, 1], x2[i, 2],
             lty = "dotted",
             col = "red")
legend("topright",
       legend = LETTERS[1:3],
       pch = 19,
       col = c("steelblue", "red", "darkgreen"),
       cex = 2,
       bty = "n")
```

Typically in machine learning, there are two clear steps, where one
first **trains** a model and then uses the model to **predict** new
outputs (class labels in this case). In kNN, these two steps are
combined into a single function call to `knn`.

